# ğŸ¦ Flappy Bird AI â€“ Deep Reinforcement Learning Agent

## ğŸ“Œ Overview

This project implements a Deep Q-Network (DQN) agent that learns to play Flappy Bird autonomously.
Using reinforcement learning techniques like experience replay and target networks, the agent improves over time by trial and error, eventually mastering the game.

## âš™ï¸ Features

    ğŸ® Autonomous Gameplay â€“ AI agent learns to play Flappy Bird without human intervention.
    
    ğŸ§  Deep Q-Network (DQN) â€“ Neural network approximates Q-values for decision-making.
    
    ğŸ”„ Experience Replay â€“ Past experiences are stored and replayed to stabilize training.
    
    ğŸ¯ Target Network Updates â€“ Improves training stability and prevents divergence.
    
    ğŸ’¾ Model Checkpointing â€“ Saves best-performing models and logs training progress.
    
    ğŸ–¥ï¸ Training & Testing Modes â€“ Train from scratch or test pre-trained models with rendering.

## ğŸ—ï¸ Architecture

      Environment (Flappy Bird Gym)
                 â†“
         Neural Network (DQN)
                 â†“
       Action Selection (Îµ-greedy)
                 â†“
       Experience Replay Memory
                 â†“
      Q-value Update + Target Network

## ğŸš€ Getting Started
    1ï¸âƒ£ Clone the Repository
    git clone <repo-link>
    cd flappy-bird-rl
    
    2ï¸âƒ£ Install Dependencies
    pip install -r requirements.txt
    
    3ï¸âƒ£ Train the Agent
    python main.py
    
    4ï¸âƒ£ Test the Agent (with rendering)
    Switch to the test() function in main function.

## ğŸ§‘â€ğŸ’» Tech Stack

    Python 3.9+
    
    PyTorch â€“ Deep Q-Network implementation
    
    Gymnasium + Flappy Bird Environment â€“ RL environment for training/testing
    
    YAML â€“ Hyperparameter configuration

## ğŸ“Š Training Workflow

    Agent starts with random actions (exploration via Îµ-greedy).
    
    Rewards are collected and stored in ReplayMemory.
    
    DQN learns from batches of past experiences.
    
    Target network updated periodically for stability.
    
    Model checkpoints saved when performance improves.
